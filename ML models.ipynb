{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h3\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "h3_l7_df = pd.read_csv('h3_l7_df_new.csv')\n",
    "h3_l7_df.drop(columns=['commercial;yes'], inplace=True)\n",
    "\n",
    "train_index=[]\n",
    "test_index=[]\n",
    "for i in range(0,h3_l7_df.shape[0]):\n",
    "    geo_location = h3.h3_to_geo(h3_l7_df.iloc[i]['id'])\n",
    "\n",
    "    if (geo_location[1]) > (-76.05): #把經度大於-76.05的 當train (東邊是train)\n",
    "        train_index.append(i)\n",
    "    else:\n",
    "        test_index.append(i)\n",
    "\n",
    "# 分割訓練集和測試集\n",
    "train_h3_l7_df = h3_l7_df.iloc[train_index]\n",
    "test_h3_l7_df = h3_l7_df.iloc[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 將 h3_l7_df 資料框中的 'id' 列移除，僅保留數據進行正規化\n",
    "h3_spatial_data = h3_l7_df.drop('id', axis=1)\n",
    "\n",
    "\n",
    "# # # 對數據進行正規化：將每個數據列的最小值調整為 0，最大值調整為 1\n",
    "normalized_spatial_data = (h3_spatial_data - h3_spatial_data.min()) / (h3_spatial_data.max() - h3_spatial_data.min())\n",
    "\n",
    "#設定OHCA正規化反函數 方便把預測結果返回原本scale\n",
    "ohca_reguli_inverse = (h3_l7_df.ohca.max()-h3_l7_df.ohca.min()) + h3_l7_df.ohca.min()\n",
    "\n",
    "# 將 DataFrame 轉換為 numpy array，並設定數據類型為 np.float64\n",
    "spatial_data = np.array(normalized_spatial_data).astype(np.float64)\n",
    "\n",
    "\n",
    "train_spatial_data = spatial_data[train_index]\n",
    "test_spatial_data = spatial_data[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分离特征和标签\n",
    "X = spatial_data[:, :-1]  # 特征\n",
    "Y = spatial_data[:, -1]   # 标签（OHCA）\n",
    "\n",
    "X_train_reguli = train_spatial_data[:, :-1] \n",
    "y_train_reguli = train_spatial_data[:, -1] \n",
    "X_test_reguli = test_spatial_data[:, :-1] \n",
    "y_test_reguli = test_spatial_data[:, -1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.25054\n",
      "[1]\tvalidation_0-rmse:0.22554\n",
      "[2]\tvalidation_0-rmse:0.21140\n",
      "[3]\tvalidation_0-rmse:0.19327\n",
      "[4]\tvalidation_0-rmse:0.18254\n",
      "[5]\tvalidation_0-rmse:0.17660\n",
      "[6]\tvalidation_0-rmse:0.17171\n",
      "[7]\tvalidation_0-rmse:0.16337\n",
      "[8]\tvalidation_0-rmse:0.15769\n",
      "[9]\tvalidation_0-rmse:0.15715\n",
      "[10]\tvalidation_0-rmse:0.15596\n",
      "[11]\tvalidation_0-rmse:0.15562\n",
      "[12]\tvalidation_0-rmse:0.15584\n",
      "[13]\tvalidation_0-rmse:0.15524\n",
      "[14]\tvalidation_0-rmse:0.15493\n",
      "[15]\tvalidation_0-rmse:0.15519\n",
      "[16]\tvalidation_0-rmse:0.15548\n",
      "[17]\tvalidation_0-rmse:0.15476\n",
      "[18]\tvalidation_0-rmse:0.15671\n",
      "[19]\tvalidation_0-rmse:0.15706\n",
      "[20]\tvalidation_0-rmse:0.15613\n",
      "[21]\tvalidation_0-rmse:0.15757\n",
      "[22]\tvalidation_0-rmse:0.15821\n",
      "[23]\tvalidation_0-rmse:0.15681\n",
      "[24]\tvalidation_0-rmse:0.15803\n",
      "[25]\tvalidation_0-rmse:0.15833\n",
      "[26]\tvalidation_0-rmse:0.15748\n",
      "[27]\tvalidation_0-rmse:0.15814\n",
      "參數之選擇: {'subsample': 1.0, 'reg_lambda': 0.01, 'reg_alpha': 0.01, 'n_estimators': 100, 'min_child_weight': 5, 'max_depth': 6, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 0.8}\n",
      "MAE of train set=  1.1932001156979297\n",
      "R² of train set=  0.9569727043687135\n",
      "MAE of test set=  7.037531109249338\n",
      "R² of test set=  0.5892521972365883\n"
     ]
    }
   ],
   "source": [
    "# 定义XGBoost模型，提前设置early_stopping_rounds\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, early_stopping_rounds=10)\n",
    "\n",
    "# 定義 XGBoost 模型，並設置基本參數與早停條件\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',  # 設置目標函數為平方誤差\n",
    "    random_state=456546,               # 設定隨機種子以確保結果可重現 42 456546 276 320\n",
    "    early_stopping_rounds=10       # 如果驗證集指標在 10 輪內無改善則提前停止\n",
    ")\n",
    "\n",
    "# 定義超參數搜索的候選值範圍\n",
    "param_dist = {\n",
    "    'max_depth': [3, 6, 10, 12],          # 樹的最大深度，控制模型的複雜度\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2], # 每次迭代的步伐大小\n",
    "    'n_estimators': [50, 100, 200, 300],  # 樹的數量\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],    # 每棵樹隨機採樣的比例\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0], # 每棵樹使用的特徵比例\n",
    "    'gamma': [0, 0.1, 0.2, 0.3],          # 控制節點分裂的最小增益\n",
    "    'reg_alpha': [0, 0.01, 0.1, 1],       # L1 正則化強度\n",
    "    'reg_lambda': [0, 0.01, 0.1, 1],      # L2 正則化強度\n",
    "    'min_child_weight': [1, 3, 5],        # 每個葉子節點的最小樣本權重\n",
    "}\n",
    "\n",
    "# 使用隨機搜尋進行超參數調整\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,                 # 基礎模型為 XGBoost\n",
    "    param_distributions=param_dist,      # 定義超參數搜索空間\n",
    "    n_iter=100,                          # 搜索 100 次\n",
    "    cv=5,                                # 使用 5 折交叉驗證\n",
    "    scoring='neg_mean_absolute_error',   # 評估指標為負的平均絕對誤差\n",
    "    n_jobs=-1,                           # 使用所有可用 CPU 加速計算\n",
    "    random_state=42                      # 設定隨機種子以確保結果可重現\n",
    ")\n",
    "\n",
    "# 訓練模型並進行超參數搜索\n",
    "random_search.fit(\n",
    "    X_train_reguli, y_train_reguli,                    # 訓練數據\n",
    "    eval_set=[(X_test_reguli, y_test_reguli)],         # 提供測試集進行早停監控\n",
    "    verbose=True                         # 在訓練過程中輸出詳細信息\n",
    ")\n",
    "\n",
    "# 提取經過超參數調整後的最佳模型 (判斷標準MAE)\n",
    "best_xgb_model = random_search.best_estimator_\n",
    "# 顯示最佳超參數\n",
    "print(\"參數之選擇:\", random_search.best_params_)\n",
    "\n",
    "\n",
    "# # 預測訓練集結果\n",
    "# y_pred_train = best_xgb_model.predict(X_train)\n",
    "# # 預測測試集結果\n",
    "# y_pred_test = best_xgb_model.predict(X_test)\n",
    "\n",
    "# 預測訓練集結果\n",
    "y_head_train = best_xgb_model.predict(X_train_reguli)*ohca_reguli_inverse\n",
    "y_train = y_train_reguli*ohca_reguli_inverse\n",
    "# 預測測試集結果\n",
    "y_head_test = best_xgb_model.predict(X_test_reguli)*ohca_reguli_inverse\n",
    "y_test = y_test_reguli*ohca_reguli_inverse\n",
    "\n",
    "\n",
    "\n",
    "#評估成效\n",
    "train_mae = np.abs(y_head_train-y_train)\n",
    "train_ans_mae = train_mae.sum()/train_mae.shape[0]\n",
    "print('MAE of train set= ',train_ans_mae)\n",
    "# 計算殘差變異\n",
    "train_ss_residual = np.sum((y_train - y_head_train) ** 2)\n",
    "# 計算總變異量\n",
    "train_ss_total = np.sum((y_train - np.mean(y_train)) ** 2)\n",
    "# 計算 R²\n",
    "train_r_squared = 1 - (train_ss_residual / train_ss_total)\n",
    "print(\"R² of train set= \", train_r_squared)\n",
    "\n",
    "test_mae = np.abs(y_head_test-y_test)\n",
    "test_ans_mae = test_mae.sum()/test_mae.shape[0]\n",
    "print('MAE of test set= ',test_ans_mae)\n",
    "# 計算殘差變異\n",
    "test_ss_residual = np.sum((y_test - y_head_test) ** 2)\n",
    "# 計算總變異量\n",
    "test_ss_total = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "# 計算 R²\n",
    "test_r_squared = 1 - (test_ss_residual / test_ss_total)\n",
    "print(\"R² of test set= \", test_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 找最佳seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "# import xgboost as xgb\n",
    "\n",
    "# # 定義要嘗試的 random seed 列表\n",
    "# seed_list = range(0,400)\n",
    "\n",
    "# best_test_r2 = -np.inf  # 初始化最佳 R²\n",
    "# best_seed = None\n",
    "# best_model = None\n",
    "# best_params = None\n",
    "\n",
    "# for seed in seed_list:\n",
    "#     print(f\"\\n=== 正在嘗試 random seed: {seed} ===\")\n",
    "    \n",
    "#     # 定義 XGBoost 模型\n",
    "#     xgb_model = xgb.XGBRegressor(\n",
    "#         objective='reg:squarederror',\n",
    "#         random_state=seed,\n",
    "#         early_stopping_rounds=10,\n",
    "#         eval_metric='mae'\n",
    "#     )\n",
    "\n",
    "#     # 定義超參數搜索範圍\n",
    "#     param_dist = {\n",
    "#         'max_depth': [3, 6, 10, 12],\n",
    "#         'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "#         'n_estimators': [50, 100, 200, 300],\n",
    "#         'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "#         'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "#         'gamma': [0, 0.1, 0.2, 0.3],\n",
    "#         'reg_alpha': [0, 0.01, 0.1, 1],\n",
    "#         'reg_lambda': [0, 0.01, 0.1, 1],\n",
    "#         'min_child_weight': [1, 3, 5],\n",
    "#     }\n",
    "\n",
    "#     # 使用隨機搜尋進行超參數調整\n",
    "#     random_search = RandomizedSearchCV(\n",
    "#         estimator=xgb_model,\n",
    "#         param_distributions=param_dist,\n",
    "#         n_iter=100,\n",
    "#         cv=5,\n",
    "#         scoring='neg_mean_absolute_error',\n",
    "#         n_jobs=-1,\n",
    "#         random_state=seed\n",
    "#     )\n",
    "\n",
    "#     # 訓練模型\n",
    "#     random_search.fit(\n",
    "#         X_train_reguli, y_train_reguli,\n",
    "#         eval_set=[(X_test_reguli, y_test_reguli)],\n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "#     # 獲取最佳模型\n",
    "#     current_model = random_search.best_estimator_\n",
    "    \n",
    "#     # 預測測試集\n",
    "#     y_head_test = current_model.predict(X_test_reguli) * ohca_reguli_inverse\n",
    "#     y_test = y_test_reguli * ohca_reguli_inverse\n",
    "    \n",
    "#     # 計算 R²\n",
    "#     ss_residual = np.sum((y_test - y_head_test) ** 2)\n",
    "#     ss_total = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "#     current_r2 = 1 - (ss_residual / ss_total)\n",
    "#     mae = np.mean(np.abs(y_test - y_head_test))\n",
    "#     print(f\"Test R²: {current_r2:.4f}\")\n",
    "#     print(f\"Test MAE: {mae:.4f}\")\n",
    "    \n",
    "#     # 檢查是否為目前最佳模型\n",
    "#     if current_r2 > best_test_r2:\n",
    "#         best_test_r2 = current_r2\n",
    "#         best_seed = seed\n",
    "#         best_model = current_model\n",
    "#         best_params = random_search.best_params_\n",
    "#         print(f\"發現新的最佳 R²! Seed: {seed}, R²: {current_r2:.4f}\")\n",
    "\n",
    "# # 輸出最終結果\n",
    "# print(\"\\n=== 最佳結果 ===\")\n",
    "# print(f\"最佳 random seed: {best_seed}\")\n",
    "# print(f\"最佳 Test R²: {best_test_r2:.4f}\")\n",
    "# print(\"最佳參數組合:\")\n",
    "# for param, value in best_params.items():\n",
    "#     print(f\"{param}: {value}\")\n",
    "\n",
    "# # 使用最佳模型進行預測\n",
    "# y_head_train = best_model.predict(X_train_reguli) * ohca_reguli_inverse\n",
    "# y_train = y_train_reguli * ohca_reguli_inverse\n",
    "# y_head_test = best_model.predict(X_test_reguli) * ohca_reguli_inverse\n",
    "# y_test = y_test_reguli * ohca_reguli_inverse\n",
    "\n",
    "# # 評估成效\n",
    "# def evaluate_performance(y_true, y_pred, set_name):\n",
    "#     mae = np.mean(np.abs(y_pred - y_true))\n",
    "#     ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "#     ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "#     r2 = 1 - (ss_residual / ss_total)\n",
    "#     print(f\"\\n{set_name} 集評估:\")\n",
    "#     print(f\"MAE: {mae:.4f}\")\n",
    "#     print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# evaluate_performance(y_train, y_head_train, \"訓練\")\n",
    "# evaluate_performance(y_test, y_head_test, \"測試\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定義要嘗試的 random seed 列表\n",
    "# seed_list = range(0,400)\n",
    "\n",
    "# best_test_mae = -np.inf  # 初始化最佳 R²\n",
    "# best_seed = None\n",
    "\n",
    "\n",
    "# for seed in seed_list:\n",
    "#     print(f\"\\n=== 正在嘗試 random seed: {seed} ===\")\n",
    "    \n",
    "#     # 定義XGBoost模型並指定參數\n",
    "#     best_xgb_model = xgb.XGBRegressor(\n",
    "#         objective='reg:squarederror',\n",
    "#         random_state=seed,\n",
    "#         early_stopping_rounds=10,\n",
    "#         # 以下是您指定的參數值，請根據需要修改這些值\n",
    "#         max_depth=2,               # 樹的最大深度\n",
    "#         learning_rate=0.05,         # 學習率\n",
    "#         n_estimators=100,          # 樹的數量\n",
    "#         subsample=0.6,            # 每棵樹隨機採樣的比例\n",
    "#         colsample_bytree=0.6,      # 每棵樹使用的特徵比例\n",
    "#         gamma=0,                # 控制節點分裂的最小增益\n",
    "#         reg_alpha=0.05,            # L1正則化強度\n",
    "#         reg_lambda=0.15,           # L2正則化強度\n",
    "#         min_child_weight=3        # 每個葉子節點的最小樣本權重\n",
    "#     )\n",
    "\n",
    "#     # 訓練模型\n",
    "#     best_xgb_model.fit(\n",
    "#         X_train_reguli, y_train_reguli,\n",
    "#         eval_set=[(X_test_reguli, y_test_reguli)],  # 提供測試集進行早停監控\n",
    "#         verbose=True\n",
    "#     )\n",
    "\n",
    "#     # 預測訓練集結果\n",
    "#     y_head_train = best_xgb_model.predict(X_train_reguli) * ohca_reguli_inverse\n",
    "#     y_train = y_train_reguli * ohca_reguli_inverse\n",
    "#     # 預測測試集結果\n",
    "#     y_head_test = best_xgb_model.predict(X_test_reguli) * ohca_reguli_inverse\n",
    "#     y_test = y_test_reguli * ohca_reguli_inverse\n",
    "    \n",
    "#     # 計算 R²\n",
    "#     ss_residual = np.sum((y_test - y_head_test) ** 2)\n",
    "#     ss_total = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "#     current_mae = np.mean(np.abs(y_test - y_head_test))\n",
    "#     print(f\"Test R²: {current_r2:.4f}\")\n",
    "#     print(f\"Test MAE: {current_mae:.4f}\")\n",
    "    \n",
    "#     # 檢查是否為目前最佳模型\n",
    "#     if current_mae > best_test_mae:\n",
    "#         best_test_mae = current_mae\n",
    "#         best_seed = seed\n",
    "#         best_model = current_model\n",
    "#         best_params = random_search.best_params_\n",
    "#         print(f\"發現新的最佳 mae²! Seed: {seed}, mae: {current_mae:.4f}\")\n",
    "\n",
    "# # 輸出最終結果\n",
    "# print(\"\\n=== 最佳結果 ===\")\n",
    "# print(f\"最佳 random seed: {best_seed}\")\n",
    "# print(f\"最佳 Test mae: {best_test_mae:.4f}\")\n",
    "# print(\"最佳參數組合:\")\n",
    "\n",
    "# # 使用最佳模型進行預測\n",
    "# y_head_train = best_model.predict(X_train_reguli) * ohca_reguli_inverse\n",
    "# y_train = y_train_reguli * ohca_reguli_inverse\n",
    "# y_head_test = best_model.predict(X_test_reguli) * ohca_reguli_inverse\n",
    "# y_test = y_test_reguli * ohca_reguli_inverse\n",
    "\n",
    "# # 評估成效\n",
    "# def evaluate_performance(y_true, y_pred, set_name):\n",
    "#     mae = np.mean(np.abs(y_pred - y_true))\n",
    "#     ss_residual = np.sum((y_true - y_pred) ** 2)\n",
    "#     ss_total = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "#     r2 = 1 - (ss_residual / ss_total)\n",
    "#     print(f\"\\n{set_name} 集評估:\")\n",
    "#     print(f\"MAE: {mae:.4f}\")\n",
    "#     print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# evaluate_performance(y_train, y_head_train, \"訓練\")\n",
    "# evaluate_performance(y_test, y_head_test, \"測試\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 指定參數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.26712\n",
      "[1]\tvalidation_0-rmse:0.25991\n",
      "[2]\tvalidation_0-rmse:0.25338\n",
      "[3]\tvalidation_0-rmse:0.24865\n",
      "[4]\tvalidation_0-rmse:0.24299\n",
      "[5]\tvalidation_0-rmse:0.23792\n",
      "[6]\tvalidation_0-rmse:0.23474\n",
      "[7]\tvalidation_0-rmse:0.22765\n",
      "[8]\tvalidation_0-rmse:0.22364\n",
      "[9]\tvalidation_0-rmse:0.21771\n",
      "[10]\tvalidation_0-rmse:0.21352\n",
      "[11]\tvalidation_0-rmse:0.21049\n",
      "[12]\tvalidation_0-rmse:0.20916\n",
      "[13]\tvalidation_0-rmse:0.20569\n",
      "[14]\tvalidation_0-rmse:0.20281\n",
      "[15]\tvalidation_0-rmse:0.20090\n",
      "[16]\tvalidation_0-rmse:0.19979\n",
      "[17]\tvalidation_0-rmse:0.19574\n",
      "[18]\tvalidation_0-rmse:0.19100\n",
      "[19]\tvalidation_0-rmse:0.18626\n",
      "[20]\tvalidation_0-rmse:0.18422\n",
      "[21]\tvalidation_0-rmse:0.18220\n",
      "[22]\tvalidation_0-rmse:0.17966\n",
      "[23]\tvalidation_0-rmse:0.17807\n",
      "[24]\tvalidation_0-rmse:0.17510\n",
      "[25]\tvalidation_0-rmse:0.17246\n",
      "[26]\tvalidation_0-rmse:0.17024\n",
      "[27]\tvalidation_0-rmse:0.16764\n",
      "[28]\tvalidation_0-rmse:0.16566\n",
      "[29]\tvalidation_0-rmse:0.16425\n",
      "[30]\tvalidation_0-rmse:0.16261\n",
      "[31]\tvalidation_0-rmse:0.15988\n",
      "[32]\tvalidation_0-rmse:0.15826\n",
      "[33]\tvalidation_0-rmse:0.15737\n",
      "[34]\tvalidation_0-rmse:0.15623\n",
      "[35]\tvalidation_0-rmse:0.15510\n",
      "[36]\tvalidation_0-rmse:0.15525\n",
      "[37]\tvalidation_0-rmse:0.15439\n",
      "[38]\tvalidation_0-rmse:0.15294\n",
      "[39]\tvalidation_0-rmse:0.15011\n",
      "[40]\tvalidation_0-rmse:0.14950\n",
      "[41]\tvalidation_0-rmse:0.14824\n",
      "[42]\tvalidation_0-rmse:0.14923\n",
      "[43]\tvalidation_0-rmse:0.14832\n",
      "[44]\tvalidation_0-rmse:0.14779\n",
      "[45]\tvalidation_0-rmse:0.14698\n",
      "[46]\tvalidation_0-rmse:0.14536\n",
      "[47]\tvalidation_0-rmse:0.14356\n",
      "[48]\tvalidation_0-rmse:0.14071\n",
      "[49]\tvalidation_0-rmse:0.13970\n",
      "[50]\tvalidation_0-rmse:0.13746\n",
      "[51]\tvalidation_0-rmse:0.13783\n",
      "[52]\tvalidation_0-rmse:0.13657\n",
      "[53]\tvalidation_0-rmse:0.13716\n",
      "[54]\tvalidation_0-rmse:0.13710\n",
      "[55]\tvalidation_0-rmse:0.13650\n",
      "[56]\tvalidation_0-rmse:0.13691\n",
      "[57]\tvalidation_0-rmse:0.13636\n",
      "[58]\tvalidation_0-rmse:0.13568\n",
      "[59]\tvalidation_0-rmse:0.13440\n",
      "[60]\tvalidation_0-rmse:0.13405\n",
      "[61]\tvalidation_0-rmse:0.13403\n",
      "[62]\tvalidation_0-rmse:0.13414\n",
      "[63]\tvalidation_0-rmse:0.13378\n",
      "[64]\tvalidation_0-rmse:0.13321\n",
      "[65]\tvalidation_0-rmse:0.13196\n",
      "[66]\tvalidation_0-rmse:0.13211\n",
      "[67]\tvalidation_0-rmse:0.13222\n",
      "[68]\tvalidation_0-rmse:0.13189\n",
      "[69]\tvalidation_0-rmse:0.13261\n",
      "[70]\tvalidation_0-rmse:0.13186\n",
      "[71]\tvalidation_0-rmse:0.13132\n",
      "[72]\tvalidation_0-rmse:0.13138\n",
      "[73]\tvalidation_0-rmse:0.13120\n",
      "[74]\tvalidation_0-rmse:0.13094\n",
      "[75]\tvalidation_0-rmse:0.13096\n",
      "[76]\tvalidation_0-rmse:0.13075\n",
      "[77]\tvalidation_0-rmse:0.13039\n",
      "[78]\tvalidation_0-rmse:0.13020\n",
      "[79]\tvalidation_0-rmse:0.12977\n",
      "[80]\tvalidation_0-rmse:0.12945\n",
      "[81]\tvalidation_0-rmse:0.12957\n",
      "[82]\tvalidation_0-rmse:0.12972\n",
      "[83]\tvalidation_0-rmse:0.12898\n",
      "[84]\tvalidation_0-rmse:0.12874\n",
      "[85]\tvalidation_0-rmse:0.12787\n",
      "[86]\tvalidation_0-rmse:0.12725\n",
      "[87]\tvalidation_0-rmse:0.12756\n",
      "[88]\tvalidation_0-rmse:0.12727\n",
      "[89]\tvalidation_0-rmse:0.12736\n",
      "[90]\tvalidation_0-rmse:0.12690\n",
      "[91]\tvalidation_0-rmse:0.12643\n",
      "[92]\tvalidation_0-rmse:0.12667\n",
      "[93]\tvalidation_0-rmse:0.12653\n",
      "[94]\tvalidation_0-rmse:0.12709\n",
      "[95]\tvalidation_0-rmse:0.12747\n",
      "[96]\tvalidation_0-rmse:0.12770\n",
      "[97]\tvalidation_0-rmse:0.12836\n",
      "[98]\tvalidation_0-rmse:0.12803\n",
      "[99]\tvalidation_0-rmse:0.12799\n",
      "MAE of train set=  1.6302655253065639\n",
      "R² of train set=  0.9254323262506389\n",
      "MAE of test set=  5.907088791436338\n",
      "R² of test set=  0.7258775970752243\n"
     ]
    }
   ],
   "source": [
    "# 定義XGBoost模型並指定參數\n",
    "best_xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=456546,\n",
    "    early_stopping_rounds=10,\n",
    "    # 以下是您指定的參數值，請根據需要修改這些值\n",
    "    max_depth=2,               # 樹的最大深度\n",
    "    learning_rate=0.05,         # 學習率\n",
    "    n_estimators=100,          # 樹的數量\n",
    "    subsample=0.6,            # 每棵樹隨機採樣的比例\n",
    "    colsample_bytree=0.6,      # 每棵樹使用的特徵比例\n",
    "    gamma=0,                # 控制節點分裂的最小增益\n",
    "    reg_alpha=0.05,            # L1正則化強度\n",
    "    reg_lambda=0.15,           # L2正則化強度\n",
    "    min_child_weight=3        # 每個葉子節點的最小樣本權重\n",
    ")\n",
    "\n",
    "# 訓練模型\n",
    "best_xgb_model.fit(\n",
    "    X_train_reguli, y_train_reguli,\n",
    "    eval_set=[(X_test_reguli, y_test_reguli)],  # 提供測試集進行早停監控\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 預測訓練集結果\n",
    "y_head_train = best_xgb_model.predict(X_train_reguli) * ohca_reguli_inverse\n",
    "y_train = y_train_reguli * ohca_reguli_inverse\n",
    "# 預測測試集結果\n",
    "y_head_test = best_xgb_model.predict(X_test_reguli) * ohca_reguli_inverse\n",
    "y_test = y_test_reguli * ohca_reguli_inverse\n",
    "\n",
    "\n",
    "#評估成效\n",
    "train_mae = np.abs(y_head_train-y_train)\n",
    "train_ans_mae = train_mae.sum()/train_mae.shape[0]\n",
    "print('MAE of train set= ',train_ans_mae)\n",
    "# 計算殘差變異\n",
    "train_ss_residual = np.sum((y_train - y_head_train) ** 2)\n",
    "# 計算總變異量\n",
    "train_ss_total = np.sum((y_train - np.mean(y_train)) ** 2)\n",
    "# 計算 R²\n",
    "train_r_squared = 1 - (train_ss_residual / train_ss_total)\n",
    "print(\"R² of train set= \", train_r_squared)\n",
    "\n",
    "test_mae = np.abs(y_head_test-y_test)\n",
    "test_ans_mae = test_mae.sum()/test_mae.shape[0]\n",
    "print('MAE of test set= ',test_ans_mae)\n",
    "# 計算殘差變異\n",
    "test_ss_residual = np.sum((y_test - y_head_test) ** 2)\n",
    "# 計算總變異量\n",
    "test_ss_total = np.sum((y_test - np.mean(y_test)) ** 2)\n",
    "# 計算 R²\n",
    "test_r_squared = 1 - (test_ss_residual / test_ss_total)\n",
    "print(\"R² of test set= \", test_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM(Different Kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel: linear\n",
      "C: 1\n",
      "Epsilon: 0.05\n",
      "MAE (Train): 2.155442200780529\n",
      "MAE (Test): 5.9113057370852164\n",
      "R² (Train): 0.9567643526560218\n",
      "R² (Test): 0.7306680741512512\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 正規化數據 (將特徵縮放到相同範圍，以提高模型的穩定性)\n",
    "scaler = StandardScaler()\n",
    "normalized_spatial_data = scaler.fit_transform(h3_spatial_data)\n",
    "\n",
    "# 設定 SVR 參數\n",
    "kernel = 'linear'  # 指定核函數\n",
    "C = 1  # 指定 C 參數\n",
    "epsilon = 0.05  # 指定 epsilon 參數\n",
    "\n",
    "# 初始化 SVR 模型\n",
    "svr = SVR(kernel=kernel, C=C, epsilon=epsilon)\n",
    "\n",
    "# 訓練模型\n",
    "svr.fit(X_train_reguli, y_train_reguli)\n",
    "\n",
    "# 進行預測並評估表現 (轉換回原始尺度)\n",
    "y_pred_train = svr.predict(X_train_reguli) * ohca_reguli_inverse\n",
    "y_pred_test = svr.predict(X_test_reguli) * ohca_reguli_inverse\n",
    "\n",
    "\n",
    "# 計算 MAE (平均絕對誤差) 和 R² (決定係數)\n",
    "mae_train = mean_absolute_error(y_train_reguli * ohca_reguli_inverse, y_pred_train)\n",
    "mae_test = mean_absolute_error(y_test_reguli * ohca_reguli_inverse, y_pred_test)\n",
    "\n",
    "r2_train = r2_score(y_train_reguli * ohca_reguli_inverse, y_pred_train)\n",
    "r2_test = r2_score(y_test_reguli * ohca_reguli_inverse, y_pred_test)\n",
    "\n",
    "\n",
    "# 輸出結果\n",
    "print(f\"Kernel: {kernel}\")\n",
    "print(f\"C: {C}\")\n",
    "print(f\"Epsilon: {epsilon}\")\n",
    "print(f\"MAE (Train): {mae_train}\")\n",
    "print(f\"MAE (Test): {mae_test}\")\n",
    "print(f\"R² (Train): {r2_train}\")\n",
    "print(f\"R² (Test): {r2_test}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
